#!/usr/bin/env python3
"""
Vulnerability Collector Module

This module provides functionality to collect and process vulnerability data
from the OSV (Open Source Vulnerabilities) database, specifically for Python packages.

Based on data_collection/collect_vuls.py functionality.
"""

import os
import sys
import json
import pickle
import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from collections import defaultdict, Counter
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse
from joblib import Parallel, delayed
from itertools import chain


# Add project paths
PROJECT_ROOT = Path(__file__).parent.parent.absolute()
sys.path.append(str(PROJECT_ROOT))
sys.path.append(str(PROJECT_ROOT / 'data_collection'))

from data_collection.logger import logger
from src.constant import CVE2ADVISORY_FILE_DATE, DATA_DIR, SUFFIX, POSSIBLE_COMMITS_DIR_DATE
from data_collection.my_utils import request_metadata_json_from_pypi, get_url_priority, get_repo_name, get_repo_url
from data_collection.get_compatable_python_version import filter_versions as filter_py_versions
from data_collection.github_utils import find_potential_commits_from_github




class VulnerabilityCollector:
    """
    Collects and processes vulnerability data from OSV database
    """
    
    def __init__(self, output_dir: Optional[Path] = None):
        """
        Initialize VulnerabilityCollector
        
        Args:
            output_dir: Directory to store collected data
        """
        self.output_dir = output_dir or (DATA_DIR / SUFFIX)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # File paths
        self.vul_file = self.output_dir / f"pypi_vuls_{datetime.now().strftime('%Y%m')}.json"
        self.cve2advisory_file = CVE2ADVISORY_FILE_DATE
        
    def download_osv_database(self, force_update: bool = False) -> bool:
        """
        Download OSV vulnerability database for PyPI packages
        
        Args:
            force_update: Force re-download even if file exists
            
        Returns:
            bool: Success status
        """
        if self.vul_file.exists() and not force_update:
            logger.info(f"OSV database already exists at {self.vul_file}")
            return True
            
        try:
            logger.info("Downloading OSV vulnerability database...")
            
            # Download PyPI vulnerabilities from OSV
            os.system("curl https://osv-vulnerabilities.storage.googleapis.com/PyPI/all.zip -o pypi.zip")
            os.system("unzip pypi.zip -d pypi_vuls")
            
            # Transform all vulnerabilities into a single JSON file
            self._data_transform("./pypi_vuls", str(self.vul_file))
            
            # Cleanup
            os.system("rm pypi.zip")
            os.system("rm -r pypi_vuls")
            
            logger.info(f"OSV database downloaded to {self.vul_file}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to download OSV database: {e}")
            return False
    
    def _data_transform(self, in_path: str, out_path: str):
        """
        Transform all vulnerability files into a single JSON file
        
        Args:
            in_path: Input directory containing vulnerability files
            out_path: Output JSON file path
        """
        import glob
        
        files = glob.iglob(in_path + "/*")
        result = []
        
        for file in files:
            try:
                with open(file, 'r') as infile:
                    result.append(json.load(infile))
            except Exception as e:
                logger.warning(f"Failed to process file {file}: {e}")
                
        with open(out_path, 'w') as output_file:
            json.dump(result, output_file, indent=4)
    
    def filter_unrelated_items(self, df: pd.DataFrame, cutoff_date: str=None) -> pd.DataFrame:
        """
        Filter out unrelated vulnerability items
        
        Args:
            df: DataFrame containing vulnerability data
            cutoff_date: string in the format of '%Y-%m-%d'
            
        Returns:
            Filtered DataFrame
        """
        logger.debug(f'Before filtering: {df.shape}')
        
        # Filter by date 
        if cutoff_date:
            cutoff_date = datetime.strptime(cutoff_date, '%Y-%m-%d')
            df['published_date'] = df['published'].apply(
                lambda x: datetime.strptime(x.split('T')[0], '%Y-%m-%d')
            )
            df = df[df['published_date'] <= cutoff_date]
            logger.info(f'After date filtering (before {cutoff_date}): {df.shape}')
        
        # Filter out malicious packages
        mal_count = df[df['id'].str.startswith('MAL')].shape[0]
        logger.debug(f"Malicious-related records: {mal_count}")
        df = df[~df['id'].str.startswith('MAL')]
        logger.info(f'After malicious filtering: {df.shape}')
        
        # Keep only GitHub Security Advisories
        df = df[df['id'].str.startswith('GHSA')]
        df = df[~((~df['id'].str.startswith('CVE')) & (df['aliases'].isna()))]
        logger.debug(f'Only keep GitHub advisory: {df.shape}')
        
        # Keep only GitHub reviewed items
        df = df[df['database_specific'].apply(
            lambda x: x.get('github_reviewed', False) == True
        )]
        logger.info(f'Only keep github_reviewed == True: {df.shape}')
        
        # Filter out withdrawn advisories
        withdrawn_count = df[df['withdrawn'].notna()].shape[0]
        logger.debug(f"Withdrawn count: {withdrawn_count}")
        df = df[df['withdrawn'].isna()]
        logger.info(f'Only keep non-withdrawn advisory: {df.shape}')
        
        logger.debug(f'After filtering: {df.shape}')
        return df
    
    def select_by_package_name(self, df: pd.DataFrame, package_name: str) -> pd.DataFrame:
        """
        Select vulnerabilities for a specific package
        
        Args:
            df: DataFrame containing vulnerability data
            package_name: Target package name
            
        Returns:
            Filtered DataFrame for the package
        """
        return df[
            df.apply(
                lambda x: any(
                    p['package']['name'].lower() == package_name.lower() 
                    for p in x['affected']
                ), 
                axis=1
            )
        ]
    
    def get_available_affected_pkgs(self, affected_pkgs: List[Dict], 
                                   filter_versions: bool = False) -> Dict[str, List[str]]:
        """
        Get available affected packages and their versions
        
        Args:
            affected_pkgs: List of affected package information
            filter_versions: Whether to filter compatible Python versions
            
        Returns:
            Dictionary mapping package names to version lists
        """
        available_pkgs = {}
        
        for affected_pkg in affected_pkgs:
            pkg_name = affected_pkg['package']['name']
            versions = affected_pkg['versions']
            
            if filter_versions:

                filter_file = DATA_DIR /SUFFIX / 'available_versions' / f'{pkg_name}.pkl'

                filtered_versions = filter_py_versions(pkg_name, versions, filter_file=filter_file, rewrite=False)
                if len(filtered_versions):
                    available_pkgs[pkg_name] = filtered_versions
            else:
                available_pkgs[pkg_name] = versions
                
        return available_pkgs
    
    def get_cve2advisory(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """
        Transform DataFrame to CVE-to-advisory mapping
        
        Args:
            df: Filtered vulnerability DataFrame
            
        Returns:
            Dictionary mapping CVE IDs to advisory information
        """
        json_data = df.to_json(orient='records')
        json_data = json.loads(json_data)
        
        cve2advisory = {}
        valid_pkg_cve_data = {}
        for item in tqdm(json_data):
            # Extract CVE ID from aliases
            cve_id = None
            cve_ids_in_alias = [alias for alias in item['aliases'] if alias.startswith('CVE')]

            if len(cve_ids_in_alias) == 1:
                cve_id = cve_ids_in_alias[0]
            else:
                nvd_urls = [ref['url'] for ref in item['references'] if
                        ref['url'].startswith('https://nvd.nist.gov')]   
                cve_id_in_refs = [nvd_url.split('/')[-1] for nvd_url in nvd_urls]  
                cve_ids = list(set(cve_ids_in_alias) & set(cve_id_in_refs))

                if len(cve_ids) == 1:
                    cve_id = cve_ids[0]

            if not cve_id:
                continue
            item['cve_id'] = cve_id         
                
            # Get affected packages with repository information
            affected_packages = item.get('affected', [])
            available_affected = {}
            
            all_affected_pypi_packages = []
            for affected_item in affected_packages:
                pkg = affected_item['package']
                if pkg['ecosystem'] != 'PyPI':
                    continue
                versions = affected_item.get('versions', [])
                all_affected_pypi_packages.append(affected_item)

            # Get available versions
            all_available_affected_pypi_packages = self.get_available_affected_pkgs(affected_pkgs = all_affected_pypi_packages, filter_versions=True)
            item['available_affected'] = all_available_affected_pypi_packages
            # if sum([len(versions) for versions in all_available_affected_pypi_packages.values()]) == 0:
            #     continue
            # else:
            valid_pkg_cve_data[cve_id]=item
            
            # Get potential commit URLs
            continue
            # for cve_id, advisory in valid_pkg_cve_data.items():
            #     # Get repository URL from package metadata
            #     refs = advisory.get('references', [])
            #     try:
            #         metadata = request_metadata_json_from_pypi(pkg_name)
            #         repo_url = None
            #         if metadata and 'info' in metadata:
            #             urls = metadata['info'].get('project_urls', {})
            #             repo_url = (urls.get('Repository') or 
            #                       urls.get('Source') or 
            #                       urls.get('Homepage'))
            #     except Exception as e:
            #         logger.warning(f"Failed to get metadata for {pkg_name}: {e}")
            #         repo_url = None
                
            #     available_affected[pkg_name] = {
            #         'versions': versions,
            #         'repo_url': repo_url
            #     }
            

        return valid_pkg_cve_data
    
    def fetch_vul_records_from_osv(self, force_update: bool = False) -> Dict[str, Dict]:
        """
        Fetch vulnerability records from OSV database
        
        Args:
            force_update: Force re-download and re-process data
            
        Returns:
            Dictionary mapping CVE IDs with available packages to advisory information,
            List of CVE IDs without available packages
        """
        # Download OSV database if needed
        if not self.vul_file.exists() or force_update:
            success = self.download_osv_database(force_update=True)
            if not success:
                raise RuntimeError("Failed to download OSV database")
        
        # Load and process vulnerability data
        logger.debug(f"Loading vulnerability data from {self.vul_file}")
        df = pd.read_json(self.vul_file)
        
        # Filter unrelated items
        df = self.filter_unrelated_items(df,'2025-06-19')
        
        # Check if cached CVE2advisory exists
        if not self.cve2advisory_file.exists() or force_update:
            logger.info("Creating CVE2advisory mapping...")
            cve2advisory = self.get_cve2advisory(df)
            
            # Save to cache
            self.cve2advisory_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cve2advisory_file, 'wb') as f:
                pickle.dump(cve2advisory, f)
        else:
            logger.info(f"Loading cached CVE2advisory mapping from {self.cve2advisory_file}...")
            with open(self.cve2advisory_file, 'rb') as f:
                cve2advisory = pickle.load(f)
        
            logger.info(f"Loaded {len(cve2advisory)} CVE records")
        return cve2advisory
    
    def evaluate_dataset_quality(self, cve2advisory: Dict[str, Dict]) -> Dict[str, int]:
        """
        Evaluate the quality and statistics of the collected dataset
        
        Args:
            cve2advisory: CVE to advisory mapping
            
        Returns:
            Dictionary containing dataset statistics
        """
        stats = {
            'total_cves': len(cve2advisory),
            'cves_with_affected_packages': 0,
            'total_affected_packages': 0,
            'cves_with_available_affected_packages': 0,
            'total_available_affected_packages': 0,
            'cves_with_references': 0,
            'total_versions': set(),
            'cves_with_possible_fixing_commits': 0
        }
        
        for cve_id, advisory in cve2advisory.items():
            try:
                # Check affected packages (from 'affected' field)
                affected = advisory.get('affected', [])
                if len(affected) > 0:
                    stats['cves_with_affected_packages'] += 1
                    stats['total_affected_packages'] += len(affected)
                
                # Check available_affected packages
                available_affected = advisory.get('available_affected', {})
                if len(available_affected)>0:
                    stats['cves_with_available_affected_packages'] += 1
                    stats['total_available_affected_packages'] += len(available_affected)
                    
                    # Count versions from available_affected packages
                    for pkg_name, versions in available_affected.items():
                        if isinstance(versions, list):
                            stats['total_versions'].update(versions)
                
                # Check references
                references = advisory.get('references', [])
                if references and len(references) > 0:
                    stats['cves_with_references'] += 1
                
                # Check possible fixing commits (check if file exists)
                possible_commits_file = Path(POSSIBLE_COMMITS_DIR_DATE) / f"{cve_id}.json"
                with open(possible_commits_file, 'r') as f:
                    possible_commits = json.load(f)
                if possible_commits_file.exists() and sum(len(commits) for commits in possible_commits.values()):
                    stats['cves_with_possible_fixing_commits'] += 1
                    
            except Exception as e:
                logger.warning(f"Error processing CVE {cve_id} in quality evaluation: {e}")
                continue
        
        # Log statistics
        stats['total_versions'] = len(stats['total_versions'])
        logger.info("Dataset Quality Statistics:")
        logger.info(f"  Total CVEs: {stats['total_cves']}")
        logger.info(f"  CVEs with affected packages: {stats['cves_with_affected_packages']}")
        logger.info(f"  Total affected packages: {stats['total_affected_packages']}")
        logger.info(f"  CVEs with available affected packages: {stats['cves_with_available_affected_packages']}")
        logger.info(f"  Total available affected packages: {stats['total_available_affected_packages']}")
        logger.info(f"  CVEs with references: {stats['cves_with_references']}")
        logger.info(f"  Total versions: {stats['total_versions']}")
        logger.info(f"  CVEs with possible fixing commits: {stats['cves_with_possible_fixing_commits']}")
        
        return stats
    
    def get_vulnerabilities_for_package(self, package_name: str, 
                                      cve2advisory: Optional[Dict] = None) -> List[Dict]:
        """
        Get all vulnerabilities for a specific package
        
        Args:
            package_name: Target package name
            cve2advisory: CVE to advisory mapping (will fetch if not provided)
            
        Returns:
            List of vulnerability records for the package
        """
        if cve2advisory is None:
            cve2advisory = self.fetch_vul_records_from_osv()
        
        package_vulnerabilities = []
        
        for cve_id, advisory in cve2advisory.items():
            affected = advisory.get('available_affected', {})
            
            if package_name.lower() in [pkg.lower() for pkg in affected.keys()]:
                package_vulnerabilities.append({
                    'cve_id': cve_id,
                    'advisory': advisory,
                    'package_info': affected.get(package_name, {})
                })
        
        return package_vulnerabilities
    
    def extract_all_possible_urls(self, advisory: Dict) -> List[str]:
        """
        Extract all possible commit URLs from an advisory
        
        Args:
            advisory: Advisory dictionary containing references
            
        Returns:
            List of potential commit URLs sorted by priority
        """
        visited_pull_ids = []
        url_result = defaultdict(list)
        # Extract commit URLs from references
        references = advisory.get('references', [])
        for ref in references:
            if isinstance(ref, dict) and 'url' in ref:
                url = ref['url']
                if url:
                    parsed_url = urlparse(url)
                    nloc = parsed_url.netloc
                    if nloc == 'github.com':
                        source, commit_urls = find_potential_commits_from_github(logger, url, visited_pull_ids)
                
                        if source:
                            url_result[source].extend(commit_urls)
    
        
        return url_result
    
    def find_possible_commits_for_advisory(self, advisory: Dict) -> List[str]:
        """
        Find possible commit URLs for a single advisory
        
        Args:
            advisory: Advisory dictionary
            
        Returns:
            List of commit URLs found
        """
        url_result = self.extract_all_possible_urls(advisory)
        
        return url_result

    def process_advisory_joblib(self,cve_id: str, advisory: Dict, rewrite_all_possible_urls: bool = False) -> Tuple[str, List[str]]:
        """
        Process a single advisory to find commits using joblib
        
        Args:
            cve_id: CVE identifier
            advisory: Advisory information dictionary
            rewrite_all_possible_urls: Whether to rewrite existing URL files
            
        Returns:
            Tuple of (cve_id, list of commit URLs)
        """
        # 1. mining possible urls from advisories
        possible_commit_file = POSSIBLE_COMMITS_DIR_DATE / f"{cve_id}.json"
        if not possible_commit_file.parent.exists():
            possible_commit_file.parent.mkdir(parents=True, exist_ok=True)
        if not rewrite_all_possible_urls and possible_commit_file.exists():
            with possible_commit_file.open('r') as f:
                extracted_commit_urls = json.load(f)
            return cve_id, extracted_commit_urls
        extracted_commit_urls = self.find_possible_commits_for_advisory(advisory)
        
        with possible_commit_file.open('w') as f:
            json.dump(extracted_commit_urls, f)
        return cve_id, extracted_commit_urls


    def collect_possible_commits(self, cve2advisory: Dict[str, Dict], 
                               n_jobs: int = 5) -> Dict[str, List[str]]:
        """
        Collect possible commits for all advisories in parallel using joblib
        
        Args:
            cve2advisory: Dictionary mapping CVE IDs to advisory information
            n_jobs: Number of parallel jobs (default: 5, following collect_commits.py pattern)
            
        Returns:
            Dictionary mapping CVE IDs to lists of possible commit URLs
        """
        logger.info(f"Starting possible commits collection for {len(cve2advisory)} advisories...")
        
        # Process advisories in parallel using joblib
        results = Parallel(n_jobs=n_jobs,backend='threading')(
            delayed(self.process_advisory_joblib)(cve_id, advisory, rewrite_all_possible_urls=False) 
            for cve_id, advisory in cve2advisory.items()
        )
        
        # Convert results to dictionary
        cve2commits = {}
        for cve_id, commits in results:
            cve2commits[cve_id] = commits
            # if commits:
            #     logger.debug(f"Found {sum(len(commits) for commits in commits.values())} commits for {cve_id}")
        
        # Log summary
        total_commits = sum(sum(len(values) for values in commits.values()) for commits in cve2commits.values())
        cves_with_commits = sum(1  for commits in cve2commits.values() if sum(len(values) for values in commits.values()))
        
        logger.info(f"Possible commits collection completed:")
        logger.info(f"  - Total CVEs processed: {len(cve2advisory)}")
        logger.info(f"  - CVEs with commits: {cves_with_commits}")
        logger.info(f"  - Total commits found for All CVES: {total_commits}")
        
        return cve2commits

    def collect_vulnerabilities(self, force_update: bool = False) -> Dict[str, Dict]:
        """
        Main method to collect vulnerability data
        
        Args:
            force_update: Force re-download and re-process data
            
        Returns:
            Dictionary mapping CVE IDs to advisory information
        """
        logger.info("Starting vulnerability collection...")
        
        try:
            # Fetch vulnerability records
            cve2advisory = self.fetch_vul_records_from_osv(force_update=force_update)
            
            
            logger.info("Vulnerability collection completed successfully")
            return cve2advisory
            
        except Exception as e:
            logger.error(f"Failed to collect vulnerabilities: {e}")
            raise
    
    def collect_vulnerabilities_with_commits(self, force_update: bool = False, 
                                           collect_commits: bool = True,
                                           n_jobs: int = 5) -> Tuple[Dict[str, Dict], Dict[str, List[str]], Dict[str, List[str]]]:
        """
        Comprehensive method to collect vulnerabilities, possible commits, and repositories
        
        Args:
            force_update: Whether to force update the vulnerability database
            collect_commits: Whether to collect possible commits
            n_jobs: Number of parallel jobs for joblib processing
            
        Returns:
            Tuple of (cve2advisory, cve2commits, cve2repos)
        """
        logger.info("Starting comprehensive vulnerability and commit collection...")
        
        try:
            # Step 1: Collect basic vulnerability data
            cve2advisory = self.collect_vulnerabilities(force_update=force_update)
            
            cve2commits = {}
            cve2repos = {}
            
            if collect_commits:
                # Step 2: Collect possible commits
                cve2commits = self.collect_possible_commits(cve2advisory, n_jobs=n_jobs)
            
            logger.info("Comprehensive collection completed successfully")

            return cve2advisory, cve2commits
            
        except Exception as e:
            logger.error(f"Failed to collect vulnerabilities with commits: {e}")
            raise
    
    def save_results(self, cve2advisory: Dict[str, Dict], 
                    cve2commits: Dict[str, List[str]] = None,
                    cve2repos: Dict[str, List[str]] = None) -> Dict[str, Path]:
        """
        Save collection results to files
        
        Args:
            cve2advisory: CVE to advisory mapping
            cve2commits: CVE to commits mapping (optional)
            cve2repos: CVE to repositories mapping (optional)
            suffix: Suffix to add to filenames
            
        Returns:
            Dictionary mapping result type to saved file path
        """
        saved_files = {}
        
        # Save advisory data
        advisory_file = self.output_dir / f"cve2advisory.pkl"
        with open(advisory_file, 'wb') as f:
            pickle.dump(cve2advisory, f)
        saved_files['advisory'] = advisory_file
        logger.info(f"Saved advisory data to {advisory_file}")
        
        return saved_files


def create_parser() -> argparse.ArgumentParser:
    """Create command line argument parser for vulnerability collector"""
    parser = argparse.ArgumentParser(
        description='Vulnerability Collector - Collect and process vulnerability data from OSV database',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                                           # Collect all vulnerabilities (default mode)
  %(prog)s --force-update                            # Collect all vulnerabilities with force update
  %(prog)s --cve CVE-2020-13757                      # View specific CVE statistics
  %(prog)s --package Django                          # Analyze specific package
  %(prog)s --collect --force-update                  # Explicit collect mode with force update
        """
    )
    
    # Main operation mode
    parser.add_argument('--collect', action='store_true',
                       help='Collect vulnerability data from OSV database (default if no other mode specified)')
    
    # Configuration options
    parser.add_argument('--cve', type=str, nargs='*',
                       help='Specific CVE IDs to view statistics for (supports multiple CVEs)')
    parser.add_argument('--package', type=str, nargs='*',
                       help='Specific package name to analyze')
    parser.add_argument('--force-update', action='store_true',
                       help='Force update cached data')
    parser.add_argument('--output', type=str,
                       help='Output file for results (JSON format)')
    parser.add_argument('--jobs', type=int, default=5,
                       help='Number of parallel jobs for commit collection (default: 5)')
    
    return parser


def main():
    """Main function for vulnerability collector with command line interface"""
    parser = create_parser()
    args = parser.parse_args()
    
    
    # If no specific operation is requested, default to collect mode
    # if not args.collect:
        # print("No operation mode specified, defaulting to --collect mode...")
    args.collect = True
    
    # Initialize collector
    collector = VulnerabilityCollector(output_dir=DATA_DIR/SUFFIX)
    
    try:
        if args.collect:
            print("Starting vulnerability collection...")
            
            # Determine collection parameters
            force_update = args.force_update
            n_jobs = args.jobs
            
            # Collect vulnerabilities with commits
            cve2advisory, cve2commits = collector.collect_vulnerabilities_with_commits(
                force_update=force_update,
                collect_commits=True,
                n_jobs=n_jobs
            )
            
            # Save results for all collected data
            saved_files = collector.save_results(cve2advisory, cve2commits)
            print(f"\nResults saved to:")
            for result_type, file_path in saved_files.items():
                print(f"  {result_type}: {file_path}")
            
            # Evaluate dataset quality for all collected data
            stats = collector.evaluate_dataset_quality(cve2advisory)
            
            # Display overall dataset quality statistics
            print(f"\n=== Overall Dataset Quality Statistics ===")
            print(f"Total CVEs collected: {len(cve2advisory)}")
            print(f"CVEs with commits: {len([cve for cve, commits in cve2commits.items() if commits])}")
            print(f"Total commits found: {sum(len(commits) for commits in cve2commits.values())}")
            
            # Print detailed statistics
            for stat_name, stat_value in stats.items():
                print(f"{stat_name}: {stat_value}")
            
            # Filter by specific CVEs for detailed analysis if requested
            if args.cve:
                print(f"\n=== Analyzing Specific CVEs: {args.cve} ===")
                filtered_cve2advisory = {
                    cve: advisory for cve, advisory in cve2advisory.items()
                    if cve in args.cve
                }
                filtered_cve2commits = {
                    cve: commits for cve, commits in cve2commits.items()
                    if cve in args.cve
                }
                
                if len(filtered_cve2advisory) == 0:
                    print(f"Warning: None of the specified CVEs {args.cve} were found in OSV(PyPI) database")
                else:
                    # Prepare JSON output for CVE analysis
                    cve_analysis_results = []
                    
                    # Show detailed information for each requested CVE
                    for cve_id in args.cve:
                        if cve_id in filtered_cve2advisory:
                            advisory = filtered_cve2advisory[cve_id]
                            commits = filtered_cve2commits.get(cve_id, [])
                            commits = list(set(chain.from_iterable(commits.values())))
                            
                            # Extract repository URL from commits
                            repository_url = None
                            if commits:
                                # Try to extract repository URL from first commit
                                try:
                                    parsed_url = urlparse(commits[0])
                                    if 'github.com' in parsed_url.netloc:
                                        path_parts = parsed_url.path.strip('/').split('/')
                                        if len(path_parts) >= 2:
                                            repository_url = f"https://github.com/{path_parts[0]}/{path_parts[1]}"
                                except:
                                    repository_url = None
                            
                            affected_packages = advisory.get('affected', [])
                            affected_packages_json = []
                            available_affected =  advisory.get('available_affected', [])
                            
                            for affected_pkg in affected_packages:
                                pkg_name = affected_pkg.get('package', {}).get('name', 'Unknown')
                                
                                # Get total versions count
                                versions = affected_pkg.get('versions', [])
                                total_versions = len(versions)
                                # Extract introduced and fixed versions from ranges
                                ranges = affected_pkg.get('ranges', [])
                                introduced_version = None
                                fixed_version = None
                                
                                if ranges:
                                    for range_info in ranges:
                                        events = range_info.get('events', [])
                                        
                                        for event in events:
                                            if 'introduced' in event:
                                                introduced_version = event.get('introduced', None)
                                            elif 'fixed' in event:
                                                fixed_version = event.get('fixed', None)
                                
                                package_info = {
                                    "name": pkg_name,
                                    "versions_affected": total_versions,
                                    "available_affected_versions": len(available_affected.get(pkg_name,[])),
                                    "introduced_version": introduced_version,
                                    "fixed_version": fixed_version,
                                    "repository": repository_url
                                }
                                affected_packages_json.append(package_info)
                            
                            # Create CVE result structure
                            cve_result = {
                                "vulnerability_collector": {
                                    "cve_id": cve_id,
                                    "summary": advisory.get('summary', 'No summary available'),
                                    "affected_packages": affected_packages_json,
                                    "commits_found": len(commits),
                                    "commit_urls": commits[:5] if commits else []  # Limit to first 5 commits
                                }
                            }
                            cve_analysis_results.append(cve_result)
                            
                            # Still print basic info for console output
                            print(f"\n--- {cve_id} ---")
                            print(f"Summary: {advisory.get('summary', 'No summary available')[:200]}...")
                            print(f"Affected packages: {len(affected_packages)}")
                            print(f"Commits found: {len(commits)}")
                        else:
                            print(f"\n--- {cve_id} ---")
                            print("Not found in collected data")
                    
                    # Save CVE analysis results to JSON file
                    if cve_analysis_results:
                        cve_output_path = DATA_DIR / SUFFIX / f"cve_analysis_results.json"
                        cve_output_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        with open(cve_output_path, 'w', encoding='utf-8') as f:
                            json.dump(cve_analysis_results, f, indent=2, ensure_ascii=False)
                        print(f"\nCVE analysis results saved to: {cve_output_path}")
                    
                    # Evaluate dataset quality for filtered CVEs
                    filtered_stats = collector.evaluate_dataset_quality(filtered_cve2advisory)
                    # print(f"\n=== Statistics for Selected CVEs ===")
                    # for stat_name, stat_value in filtered_stats.items():
                    #     print(f"{stat_name}: {stat_value}")
            
            # Package-specific analysis if requested
            if args.package:
                print(f"\n=== Package Analysis: {args.package} ===")
                vulnerabilities = collector.get_vulnerabilities_for_package(args.package, cve2advisory)
                
                print(f"Found {len(vulnerabilities)} vulnerabilities for {args.package}")
                
                # Prepare JSON output for package analysis
                package_analysis_results = []
                
                # Process all vulnerabilities for the package
                for vuln in vulnerabilities:
                    cve_id = vuln['cve_id']
                    advisory = vuln['advisory']
                    summary = advisory.get('summary', 'No summary available')
                    commits = cve2commits.get(cve_id, [])
                    commits = list(set(chain.from_iterable(commits.values())))
                    
                    # Extract repository URL from commits
                    repository_url = None
                    if commits:
                        try:
                            from urllib.parse import urlparse
                            parsed_url = urlparse(commits[0])
                            if 'github.com' in parsed_url.netloc:
                                path_parts = parsed_url.path.strip('/').split('/')
                                if len(path_parts) >= 2:
                                    repository_url = f"https://github.com/{path_parts[0]}/{path_parts[1]}"
                        except:
                            repository_url = None
                    
                    # Extract affected package information
                    affected_packages = advisory.get('affected', [])
                    target_package_info = None
                    
                    for affected_pkg in affected_packages:
                        pkg_name = affected_pkg.get('package', {}).get('name', 'Unknown')
                        if pkg_name.lower() == args.package.lower():
                            # Get version information
                            versions = affected_pkg.get('versions', [])
                            total_versions = len(versions)
                            
                            # Extract introduced and fixed versions from ranges
                            ranges = affected_pkg.get('ranges', [])
                            introduced_version = None
                            fixed_version = None
                            
                            if ranges:
                                for range_info in ranges:
                                    events = range_info.get('events', [])
                                    for event in events:
                                        if 'introduced' in event:
                                            introduced_version = event.get('introduced', None)
                                        elif 'fixed' in event:
                                            fixed_version = event.get('fixed', None)
                            
                            target_package_info = {
                                "name": pkg_name,
                                "versions_affected": total_versions,
                                "introduced_version": introduced_version,
                                "fixed_version": fixed_version,
                                "repository": repository_url
                            }
                            break
                    
                    # Create vulnerability result structure
                    vuln_result = {
                        "vulnerability_collector": {
                            "cve_id": cve_id,
                            "summary": summary,
                            "affected_packages": [target_package_info] if target_package_info else [],
                            "upstream_package": {
                                "name": args.package,
                                "version": "latest"  # Could be enhanced to get actual version
                            },
                            "commits_found": len(commits),
                            "commit_urls": commits[:5] if commits else []
                        }
                    }
                    package_analysis_results.append(vuln_result)
                
                # Show examples with commits (console output)
                for vuln in vulnerabilities[:3]:  # Show first 3
                    cve_id = vuln['cve_id']
                    summary = vuln['advisory']['summary'][:100] if vuln['advisory'].get('summary') else 'No summary'
                    commits = cve2commits.get(cve_id, [])
                    commits = list(set(chain.from_iterable(commits.values())))

                    print(f"\n  {cve_id}: {summary}...")
                    print(f"    Commits found: {len(commits)}")
                
                # Save package analysis results to JSON file
                if package_analysis_results:
                    package_output_path = DATA_DIR / SUFFIX / f"package_analysis_{args.package}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    package_output_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with open(package_output_path, 'w', encoding='utf-8') as f:
                        json.dump(package_analysis_results, f, indent=2, ensure_ascii=False)
                    print(f"\nPackage analysis results saved to: {package_output_path}")
            
            # Save to custom output file if specified
            if args.output:
                output_path = Path(args.output)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Determine which data to save based on filters
                output_cve2advisory = cve2advisory
                output_cve2commits = cve2commits
                
                # If specific CVEs were requested, include both full and filtered data
                if args.cve:
                    filtered_cve2advisory = {
                        cve: advisory for cve, advisory in cve2advisory.items()
                        if cve in args.cve
                    }
                    filtered_cve2commits = {
                        cve: commits for cve, commits in cve2commits.items()
                        if cve in args.cve
                    }
                
                output_data = {
                    'metadata': {
                        'collection_date': datetime.now().isoformat(),
                        'total_cves': len(cve2advisory),
                        'cves_with_commits': len([cve for cve, commits in cve2commits.items() if commits]),
                        'total_commits': sum(len(commits) for commits in cve2commits.values()),
                        'force_update': args.force_update,
                        'requested_cves': args.cve,
                        'target_package': args.package
                    },
                    'full_dataset': {
                        'cve2advisory': output_cve2advisory,
                        'cve2commits': output_cve2commits,
                        'statistics': stats
                    }
                }
                
                # Add filtered data if CVEs were specified
                if args.cve and len(filtered_cve2advisory) > 0:
                    filtered_stats = collector.evaluate_dataset_quality(filtered_cve2advisory)
                    output_data['filtered_dataset'] = {
                        'cve2advisory': filtered_cve2advisory,
                        'cve2commits': filtered_cve2commits,
                        'statistics': filtered_stats
                    }
                
                with open(output_path, 'w') as f:
                    json.dump(output_data, f, indent=2, default=str)
                print(f"\nComplete results saved to: {output_path}")
                
    except KeyboardInterrupt:
        logger.info("Collection interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Collection failed: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()